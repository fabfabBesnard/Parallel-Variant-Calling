#!/usr/bin/env nextflow

def helpMessage() {
    log.info nfcoreHeader()
    log.info"""
    Usage:

    The typical command for running the pipeline is as follows:
      ./nextflow run script/VariantCaller.nf -profile docker_conda--reads "/data/reads/*_{1,2}.fq.gz" --genomefasta data/Physcomitrella_patens.Phypa_V3.dna.toplevel.fa --sampletable data/table -profile psmn -resume --annotationname Physcomitrella_patens --outdir analyse_mousse
    (Please avoid to use dot in file name, use underscore instead.)

    You can provide these following argument in the command line or you can modify the configuration file. 
    
    Required arguments:
        - `-profile` : + profile id (string). the profile adapted to your computing environment, deined in the config file (available: psmn or singularity)

        - `--genomefasta` : + path/to/file. Full path to the file of the reference genome (.fa or .fasta or .gz)

        - `--reads` : + path/to/files. Full path to directory and name of reads in fastq.gz. Only one argument is accepted, so you should use a pattern to select several files. Symbolic links are accepted, so you can group symlinks to sequencing files of a cohort to analyze in a dedicated folder of your analysis.
        Example: "Sequence* _{1,2}.fastq" ( `{1,2}` for paired reads ). (Use quotes "" to ensure the correct interpretation of pattern)

              or 

        - `--readsinbam` : + path/to/files. Full path to directory and name of your reads in .bam. Only one argument is accepted, so you should use a pattern to select several files. You can provide your own bam from different experimentation with different readgroup.
        Example: "Sequence*.bam" (Use quotes "" to ensure the correct interpretation of pattern)

        - `--annotationname` : + name (string). Name of the organism (with a underscore instead of space) either Arabidopsis_thaliana, Physcomitrella_patens, Caenorhabditis_elegans, Caenorhabditis_briggsae, Populus_trichocarpa, Saccharomyces_cerevisiae, Zea_mays, Drosophila_melanogaster, Schizosaccharomyces_pombe

        - `--annotationgff` : If your model organism is not allready included, you can provide a annotation file. Full path to file of annotation genome (.gff format)


    Optional :

        - `--vqsr_file` : + path/to/file. You can provide a reference variant file (.vcf) in order to apply a variant recalibration score with [GATK](https://gatk.broadinstitute.org/hc/en-us/articles/360035531612-Variant-Quality-Score-Recalibration-VQSR-). Note that this file must contain a lot of highly validated trusty variants to ensure a good performance of the classification algorithms. For exemple [Arabidopsis_thaliana]( https://1001genomes.org/data/GMI-MPI/releases/v3.1/).

        - `--sampletable`: + path/to/file. In order to simplify result file naming, you can provide a table of correspondance between your regular sample names and the (often complicated) names of corresponding read (fastq) files in csv format. Example:

        - `--outdir` : + path/to/destination/directory. Name of the main result directory generated by the analysis. Default: './results'

        - `--ploidy` : + numeric. Number of chromosome copy. Default: 1 (1 or 2)

        - `--minglobalqual` : + numeric. (only for short indel and snp) Minimal cutoff threshold of global quality per variant (for all sample). Default: 200

        - `--mindepth` : + numeric. (only for short indel and snp) Minimal cutoff threshold of depth (number of reads) for a variant per sample. Default: 4

        - `--sample` : + float between 0 and 1. Create a randomly a sample files from your reads data and the float you give and use them for the rest of the pipeline. Default: false

        - `-resume` : (nothing to add). With this flag, previously generated files from other analysis that are strictly identical to this new worflow will be retrieved from the cache, save computation time and ressource !

        - `with-report`: +path/to/destination. Output an html execution report of the workflow (https://www.nextflow.io/docs/latest/tracing.html#execution-report)

    help message:
      --help                        Print help message
    """
      .stripIndent()
  }

///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                SET UP CONFIGURATION VARIABLES                       -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////


//params.help="False"
// Show help message
if (params.help) {
    helpMessage()
    exit 0
}

///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                       HEADER LOG INFO                               -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

// Header log info
log.info nfcoreHeader()
def summary = [:]
summary['reads']                 = params.reads ?: 'Not supplied'
summary['readsinbam']            = params.readsinbam ?: 'Not supplied'
summary['genomefasta']           = params.genomefasta  ?: 'Not supplied'
summary['Ploidy']                = params.ploidy
summary['Config Profile']        = workflow.profile
summary['annotationgff']         = params.annotationgff  ?: 'Not supplied'
summary['annotationname']        = params.annotationname
summary['sampletable']           = params.sampletable  ?: 'Not supplied'
summary['minglobalqual']         = params.minglobalqual
summary['mindepth']              = params.mindepth
summary['VQSR']                  = params.vqsrfile  ?: 'Not supplied'
summary['Output']                = params.outdir
summary['sample']                = params.sample  ?: 'Not supplied'
log.info summary.collect { k,v -> "${k.padRight(20)}: $v" }.join("\n")
log.info "-\033[2m-------------------------------------------------------------------------\033[0m-"

///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                          STEP   : INDEX                             -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

if (params.genomefasta) {
        Channel
            .fromPath( params.genomefasta )
            .ifEmpty { error "Cannot find any file matching: ${params.genomefasta}" }
            .into { fasta_file ; fasta_VQSR ;
            fasta_file2GATK; 
            fasta3;fasta_variantmetric; 
            fasta_variantcalling ; fasta_joingvcf; 
            fasta_extract_Extract_SNP_VQSR ; fasta_Extract_INDEL_VQSR ;
            fasta_BaseRecalibrator ; 
            fasta_dict ; fasta_snpeff ; fasta_Snpeff_variant_effect ; fasta_Snpeff_variant_effect2
            fasta_Structural_Variant_calling_GATK ; fasta_Structural_Variant_calling_GATK_prepare ;  fasta_pindel ; fasta_cnv; fasta_metasv}

            process Create_genome_bwa_index {
              label "bwa1"
              tag "$fasta.simpleName"
              publishDir "${params.outdir}/mapping/index/", mode: 'copy'

              input:
                file fasta from fasta_file

              output:
                file "${fasta.baseName}.*" into index_files 
                file "*_bwa_report.txt" into index_files_report

              script:
                """
                bwa index -p ${fasta.baseName} ${fasta} \
                &> ${fasta.baseName}_bwa_report.txt
                """

              }
}

if (params.sample){ //mettre un décimal pour le float
                                             
  process Empty_sample_dir {
    script :

      """
      #! python3
      import os

      if os.path.exists("${params.outdir}/sample"):
        print("Sample Directory removed for a new one")
        os.system("rm -rf ${params.outdir}/sample")
      else :
        print("No Sample Directory yet")

      """
  }
    
  //If option sample is true we read the sample option and create the channel for sampling process
  if (params.reads) {
    Channel
      .fromFilePairs( params.reads, size:2 )
      .ifEmpty{ error "Cannot find any file matching: ${params.reads}" }
      .set { into_file }
    //We read the sample value to get the correct part of the reads file
    Channel
      .value(params.sample)
      .set {sample_var}

    //fastq_sample = Channel.fromPath("$params.scriptdir/fastq_sample.py")

    process Sampling {
      tag "$pair_id"
      publishDir("${params.outdir}/sample") //Put the output file into the sample repertory

      input :
        set pair_id , file(reads_sample) from into_file //Switch de fromFilePair into variable pair_id and reads, reads contain the file and pair_id the pattern
        val spl from sample_var

      output :
        set pair_id, file("${pair_id}*_sampled.fastq") into fastqgz
        file("${pair_id}*_sampled.fastq") into fastqgz_fastqc
        val pair_id into pair_id_for_breakdancer, pair_id_for_lumpy

      script :

      """
      python3 $projectDir/fastq_sample.py ${spl} ${reads_sample[0]} ${reads_sample[1]} ${reads_sample[0]}_sampled.fastq ${reads_sample[1]}_sampled.fastq
      """
    }
  }
  /*Channel
    .fromPath("${params.outdir}/sample/*" )
    .set{ fastqgz_fastqc}*/
}
else {
    //If option sample is false we use the reads file for the rest of the script
    Channel
        .fromFilePairs( params.reads, size:2 )
        .set { fastqgz }
    Channel
        .fromPath(params.reads )
        .set{ fastqgz_fastqc}
}

process Fastqc {
label 'fastqc'
tag "$read"

input:
file read from fastqgz_fastqc

output:
file "*.{zip,html}" into fastq_report_files

script:

"""
fastqc --quiet --threads ${task.cpus} --format fastq --outdir ./ \
          ${read}
"""
}


///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                          STEP   : MAPPING                           -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

//Change les noms des pair_id avec les noms des echantillons directement dans la premiere channel
<<<<<<< HEAD
if (params.sampletable) {
      Channel
        .fromPath( params.sampletable )
        .splitCsv( skip: 0)
        .join(fastqgz)
        .set{fastqsamplename}

      process Mapping_reads_and_add_sample_name {
        label 'bwa'
        tag "$sample_id"

        input:
        set pair_id,sample_id, file(reads) from fastqsamplename
        file index from index_files.collect()

        output:
        set sample_id, "${sample_id}.sam" into sam_files
        set sample_id, "${sample_id}_bwa_report.txt" into mapping_report_files

        script:
        index_id = index[0].baseName // Point to Reference genome (See process bwa1 or "Create_genome_bwa_index")
        """
        bwa mem -t ${task.cpus} \
        -aM ${index_id} ${reads[0]} ${reads[1]} \
        > ${sample_id}.sam 2> ${sample_id}_bwa_report.txt
        """
      }
    }
    else{
=======
>>>>>>> debut du multiple merge
      process Mapping_reads {
        label 'bwa'
        tag "$pair_id"

        input:
        set pair_id,file(reads) from fastqgz
        file index from index_files.collect()

        output:
<<<<<<< HEAD
        set pair_id, "${pair_id}.sam" into sam_files
        set pair_id, "${pair_id}_bwa_report.txt" into mapping_report_files
=======
        set pair_id, "${pair_id}.sam" into sam_files, sam_files_test
        set pair_id, "${pair_id}_bwa_report.txt" into mapping_repport_files
>>>>>>> debut du multiple merge

        script:
        index_id = index[0].baseName
        """
        bwa mem -t ${task.cpus} \
        -aM ${index_id} ${reads[0]} ${reads[1]} \
        -o ${pair_id}.sam &> ${pair_id}_bwa_report.txt
        """
      }

    ///////////////////////////////////////////////////////////////////////////////
    ///////////////////////////////////////////////////////////////////////////////
    /* --                                                                     -- */
    /* --                       STEP   : SAM to BAM                           -- */
    /* --                                                                     -- */
    ///////////////////////////////////////////////////////////////////////////////
    ///////////////////////////////////////////////////////////////////////////////

  
    process Sam_to_bam {
      label 'samtools'
      tag "$pair_id"
      publishDir "${params.outdir}/mapping", mode: 'copy'

      input:
      set pair_id, "${pair_id}.sam" from sam_files

      output:
      set pair_id, "${pair_id}.bam" into bam_files

      script:
      """
      samtools view -buS ${pair_id}.sam | samtools sort - -o ${pair_id}.bam
      """
    }


    ///////////////////////////////////////////////////////////////////////////////
    ///////////////////////////////////////////////////////////////////////////////
    /* --                                                                     -- */
    /* --                       STEP   : READ GROUPS                          -- */
    /* --                                                                     -- */
    ///////////////////////////////////////////////////////////////////////////////
    ///////////////////////////////////////////////////////////////////////////////

    //https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard-
    if (params.sampletable){

      Channel
        .fromPath( params.sampletable )
        .splitText()
        .splitCsv()
        .groupTuple(by:1)
        .view()
        .set{fastqsamplename}

      process Add_ReadGroup {
        label 'picardtools'
        tag "$pair_id"
      
        input:
        set pair_id, bam_file from bam_files

        output:
        file "${pair_id}.bam" into bam_files_RG

        script:

          """
          picard AddOrReplaceReadGroups \
          -I ${bam_file} \
          -O "${pair_id}.bam" \
          -RGID ${pair_id} \
          -RGLB lib1 \
          -RGPL illumina \
          -RGPU unit1 \
          -RGSM ${pair_id}
          """
      }

      process MarkDuplicates_and_Merge{
        label 'picardtools'
        tag "$sample_id"

        input:
        set pair_id, val(sample_id) from fastqsamplename
        file bam_file from bam_files_RG.collect()

        output:
        set sample_id, "${sample_id}_readGroup_MarkDuplicates.bam" into bam_files_RG_MD , bam_for_strartingstrain 
        set sample_id, "${sample_id}_marked_dup_metrics.txt" into picardmetric_files

        script:
        bash_array = ""
        for (id in pair_id){
          bash_array = "-I "+id+".bam "+bash_array
        }
        bash_array = bash_array.substring(0, bash_array.length() - 1)
        """
        picard MarkDuplicates \
          $bash_array \
          -O ${sample_id}_readGroup_MarkDuplicates.bam \
          -M ${sample_id}_marked_dup_metrics.txt
        """
      }

    }
    else{
    process Add_ReadGroup_and_MarkDuplicates_bam {
      label 'picardtools'
      tag "$pair_id"

      input:
      set pair_id, bam_file from bam_files

      output:
      set pair_id, "${pair_id}_readGroup_MarkDuplicates.bam" into bam_files_RG_MD , bam_for_strartingstrain 
      set pair_id, "${pair_id}_marked_dup_metrics.txt" into picardmetric_files

      script:
      """
      picard AddOrReplaceReadGroups \
          -I ${bam_file} \
          -O "${pair_id}.bam" \
          -RGID ${pair_id} \
          -RGLB lib1 \
          -RGPL illumina \
          -RGPU unit1 \
          -RGSM ${pair_id}

      picard MarkDuplicates \
          -I "${pair_id}.bam" \
          -O "${pair_id}_readGroup_MarkDuplicates.bam" \
          -M "${pair_id}_marked_dup_metrics.txt"
      """

      /*"""
      PicardCommandLine AddOrReplaceReadGroups \
          I=${bam_file} \
          O="${pair_id}.bam" \
          RGID=${pair_id} \
          RGLB=lib1 \
          RGPL=illumina \
          RGPU=unit1 \
          RGSM=${pair_id}
      PicardCommandLine MarkDuplicates \
          I="${pair_id}.bam" \
          O="${pair_id}_readGroup_MarkDuplicates.bam" \
          M="${pair_id}_marked_dup_metrics.txt"
      """*/
    }
  }

///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                   STEP   : ufilter and Get basics statistics        -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

process Filtering_and_Indexing_bam {
  label 'samtools1'
  tag "$pair_id"

  input:
  set pair_id, file(bam_RD_MD) from bam_files_RG_MD

  output:
  set pair_id, "${pair_id}_ufilter.bam", "${pair_id}_ufilter.bam.bai" into bam_files_RG_MD_filter,filtered_bam_pindel, bam2GATK , bam2GATK_SV, filtered_bam_files_breakdancer
  set pair_id, "${pair_id}.txt" into flagstat_files
  set pair_id, "${pair_id}_ufilter.bam.bai" into bam_index_samtools
  set pair_id, "${pair_id}.bam",  "${pair_id}.bam.bai" into non_filtered_bam_pindel , bam_for_lumpy_1,bam_for_lumpy_2, non_filtered_bam_files_breakdancer, non_filtered_bam_files_cnvnator, non_filtered_bam_files_metasv
  file "${pair_id}.bam" into bam_files_cov

  script:

  """
  samtools view -hu -F4 ${bam_RD_MD} | samtools view -hu -F256 - | samtools view -hb -f3 - > ${pair_id}_ufilter.bam
	samtools flagstat ${pair_id}_ufilter.bam > ${pair_id}.txt
  samtools index ${pair_id}_ufilter.bam ${pair_id}_ufilter.bam.bai
 
  #index pour bam avant filtration
  cp ${bam_RD_MD} ${pair_id}.bam
  samtools index ${pair_id}.bam ${pair_id}.bam.bai
  """
}

if (params.readsinbam) {
          Channel
            .fromFilePairs(params.readsinbam, size:1)
            .ifEmpty{ error "Cannot find any file matching: ${params.reads}" }
            .set{ bam_files_RG_MD }


///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                   STEP   : ufilter and Get basics statistics        -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

process Filtering_and_Indexing_bam_allready_process {
  label 'samtools'
  tag "$pair_id"

  input:
  set pair_id, bam_RD_MD from bam_files_RG_MD

  output:
  set pair_id, "${pair_id}_ufilter.bam", "${pair_id}_ufilter.bam.bai" into bam_files_RG_MD_filter,filtered_bam_pindel, bam2GATK , bam2GATK_SV, filtered_bam_files_breakdancer
  set pair_id, "${pair_id}.txt" into flagstat_files
  set pair_id, "${pair_id}_ufilter.bam.bai" into bam_index_samtools
  set pair_id, "${pair_id}.bam",  "${pair_id}.bam.bai" into non_filtered_bam_pindel , bam_for_lumpy_1,bam_for_lumpy_2, non_filtered_bam_files_cnvnator, non_filtered_bam_files_breakdancer , non_filtered_bam_files_metasv
  file "${pair_id}.bam" into bam_files_cov

  script:
  """
  samtools view -hu -F4 ${bam_RD_MD[0]} | samtools view -hu -F256 - | samtools view -hb -f3 - > ${pair_id}_ufilter.bam
	samtools flagstat ${pair_id}_ufilter.bam > ${pair_id}.txt
  samtools index ${pair_id}_ufilter.bam ${pair_id}_ufilter.bam.bai
 
  #index pour bam avant filtration
  cp ${bam_RD_MD[0]} ${pair_id}.bam
  samtools index ${pair_id}.bam ${pair_id}.bam.bai
  """
}
}

process Coverage {
  label 'samtools'

  input:
  file bam from bam_files_cov.collect()

  output:
  file "Coverage_mqc.csv" into cov_multiqc

  script:
  """
  #Coverage stat
  echo "Sample, Coverage" > Coverage_mqc.csv

  for bam in \$(ls *.bam)
    do
       cov=\$(samtools depth \$bam  |  awk '{sum+=\$3} END { print sum/NR}')
       echo \$bam "," \$cov >> Coverage_mqc.csv
    done
  """
}


if (params.annotationgff) {
        Channel
            .fromPath( params.annotationgff )
            .ifEmpty { error "Cannot find any file matching: ${params.annotationgff}" }
            .set{ annotation }
}



///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --               STEP   : Variant calling GATK and get metrics         -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

//https://gatk.broadinstitute.org/hc/en-us/articles/360037057932-BuildBamIndex-Picard-

process Create_ref_index {
  label 'samtools'
  tag "$fasta"

  input:
  file fasta from fasta_file2GATK

  output:
  file "*" into fasta_fai , fasta_fai_VQSR, fasta_fai_Structural_Variant_calling_GATK,fasta_fai_Structural_Variant_calling_GATK_prepare, fasta_fai_variantmetric , fasta_fai_gvcftovcf ,fasta_fai_gvcftovcf_after_bqsr, fasta_fai_extract_Extract_SNP_VQSR ,fasta_fai_Extract_INDEL_VQSR , fasta_fai_BaseRecalibrator , fasta_fai_after_bqsr , fasta_fai_pindel , fastafai_metasv

  script:
  """
  samtools faidx $fasta 
  """
}

process Create_ref_dictionary {
  label 'gatk'
  tag "$fasta"

  input:
  file fasta from  fasta_dict

  output:
  file "*.dict" into fasta_dict_Variant_calling, fasta_dict_VQSR, fasta_dict_Structural_Variant_calling_GATK,fasta_dict_Structural_Variant_calling_GATK_prepare, fasta_dict_Variant_metric , fasta_dict_Gvcf_to_vcf , fasta_dict_Gvcf_to_vcf_after_bqsr , fasta_dict_extract_Extract_SNP_VQSR,fasta_dict_Extract_INDEL_VQSR,fasta_dict_extract_after_bqsr , fasta_dict_BaseRecalibrator , fasta_dict_Variant_calling_after_bqsr

  script:

  """
  gatk  --java-options "-Xmx30g" CreateSequenceDictionary -R $fasta
  """

}

//https://gatk.broadinstitute.org/hc/en-us/articles/360035890411-Calling-variants-on-cohorts-of-samples-using-the-HaplotypeCaller-in-GVCF-mode

/*  --sample-ploidy / -ploidy

Ploidy (number of chromosomes) per sample. For pooled data, set to (Number of samples in each pool * Sample Ploidy).
Sample ploidy - equivalent to number of chromosomes per pool. In pooled experiments this should be = # of samples in pool * individual sample ploidy

int  2  [ [ -∞  ∞ ] ]  */


process Variant_calling {
  label 'gatk'
  tag "$pair_id"
  publishDir "${params.outdir}/variant/", mode: 'copy'

  input:
  set pair_id, bam_file, bam_file_index from bam2GATK
  file fasta_fai from fasta_fai.collect()
  file fasta from fasta_variantcalling.collect()
  file fasta_dict from fasta_dict_Variant_calling.collect()
  val p from params.ploidy

  output:
  set pair_id, "${pair_id}.g.vcf.gz" into gvcf
  file "*.g.vcf.gz" into gvcf2metrics

  script:
  """
  gatk --java-options "-Xmx${task.memory.giga}g" HaplotypeCaller \
  -R $fasta \
  -I $bam_file \
  -O "./${pair_id}.g.vcf.gz" \
  -ploidy $p \
  -ERC GVCF
  """
}


///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                   STEP   : Join gvcf into vcf                       -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

//https://gatk.broadinstitute.org/hc/en-us/articles/360056967892-VariantEval-BETA-




process Join_Gvcf_to_vcf {
  label 'gatk'
  tag "and compute metrics"
  publishDir "${params.outdir}/variant/", mode: 'copy'

  input:
  file file_gvcf from gvcf.collect()
  file fasta_fai from fasta_fai_gvcftovcf
  file fasta from fasta_joingvcf
  file fasta_dict from fasta_dict_Gvcf_to_vcf

  output:
  file "final.vcf.gz" into vcf_for_indel , vcf_for_snp
  file "*.txt" into gatkmetric_files

  script:
  """
  ## make list of input variant files
  ls *g.vcf.gz > myvcf.list

  for input in \$(cat myvcf.list)
  do
    gatk --java-options "-Xmx${task.memory.giga}g" IndexFeatureFile -I \$input
  done 
  
  gatk --java-options "-Xmx${task.memory.giga}g" CombineGVCFs \
  -R $fasta \
  --variant myvcf.list \
  -O combined.g.vcf.gz

  gatk --java-options "-Xmx${task.memory.giga}g" GenotypeGVCFs \
  -R $fasta \
  -V combined.g.vcf.gz \
  -O final.vcf.gz

  gatk --java-options "-Xmx${task.memory.giga}g" VariantEval  \
   -R $fasta \
   -O "GATK_metrics.txt"\
   -EV CompOverlap -EV IndelSummary -EV CountVariants -EV MultiallelicSummary \
   --eval final.vcf.gz
  """
}

///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                   STEP   : Variant filtering                        -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

// https://gatk.broadinstitute.org/hc/en-us/articles/360035532412-Can-t-use-VQSR-on-non-model-organism-or-small-dataset
//https://gatk.broadinstitute.org/hc/en-us/articles/360036350452-VariantFiltration
process Extract_SNP_and_filtering {
  label 'gatk'
  tag "$file_vcf"
  publishDir "${params.outdir}/variant/", mode: 'copy'

  input:
  file file_vcf from vcf_for_snp
  file fasta_fai from fasta_fai_extract_Extract_SNP_VQSR
  file fasta from fasta_extract_Extract_SNP_VQSR
  file fasta_dict from fasta_dict_extract_Extract_SNP_VQSR

  output:
  file "filtered_snps.vcf" into vcf_snp , snp_files, snp_for_stat
  file "raw_snps.vcf" into rawsnp

  script:
  """
  gatk --java-options "-Xmx${task.memory.giga}g" IndexFeatureFile \
     -I $file_vcf
  
  gatk --java-options "-Xmx${task.memory.giga}g" SelectVariants \
      -R $fasta \
      -V $file_vcf \
      --select-type-to-include SNP \
      -O raw_snps.vcf

  gatk --java-options "-Xmx${task.memory.giga}g" VariantFiltration \
        -R $fasta \
        -V raw_snps.vcf \
        -O pre_filtered_snps.vcf \
        -filter-name "QD_filter" -filter "QD < 2.0" \
        -filter-name "FS_filter" -filter "FS > 60.0" \
        -filter-name "MQ_filter" -filter "MQ < 40.0" \
        -filter-name "SOR_filter" -filter "SOR > 4.0" \
        -filter-name "MQRankSum_filter" -filter "MQRankSum < -12.5" \
        -filter-name "ReadPosRankSum_filter" -filter "ReadPosRankSum < -8.0"

  gatk --java-options "-Xmx${task.memory.giga}g" SelectVariants \
      -R $fasta \
      -V pre_filtered_snps.vcf \
      --exclude-filtered \
      -O filtered_snps.vcf
  """
}

process Extract_INDEL_and_filtering {
  label 'gatk'
  tag "$file_vcf"
  publishDir "${params.outdir}/variant/", mode: 'copy'

  input:
  file file_vcf from vcf_for_indel
  file fasta_fai from fasta_fai_Extract_INDEL_VQSR
  file fasta from fasta_Extract_INDEL_VQSR
  file fasta_dict from fasta_dict_Extract_INDEL_VQSR

  output:
  file "filtered_indels.vcf" into indel , indel_files , indel_for_stat , testgatkmetasv
  file "raw_indels.vcf" into rawindel

  script:
  """
  gatk --java-options "-Xmx${task.memory.giga}g" IndexFeatureFile \
     -I $file_vcf
  
  gatk --java-options "-Xmx${task.memory.giga}g" SelectVariants \
      -R $fasta \
      -V $file_vcf \
      --select-type-to-include INDEL \
      -O raw_indels.vcf

  gatk --java-options "-Xmx${task.memory.giga}g" VariantFiltration \
        -R $fasta \
        -V raw_indels.vcf \
        -O pre_filtered_indels.vcf \
        -filter-name "QD_filter" -filter "QD < 2.0" \
        -filter-name "FS_filter" -filter "FS > 200.0" \
        -filter-name "SOR_filter" -filter "SOR > 10.0" 

  gatk --java-options "-Xmx${task.memory.giga}g" SelectVariants \
      -R $fasta \
      -V pre_filtered_indels.vcf \
      --exclude-filtered \
      -O filtered_indels.vcf
  """
}



///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                         STEP   : VQSR                               -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
 //https://gatk.broadinstitute.org/hc/en-us/articles/360051306591-ApplyVQSR
 //https://gatk.broadinstitute.org/hc/en-us/articles/360036510892-VariantRecalibrator

//ext_spec = Channel.fromPath("$params.scriptdir/extract_specific.py")

if (params.vqsrfile) {

  Channel
            .fromPath( params.vqsrfile )
            .ifEmpty { error "Cannot find any file matching: ${params.vqsrfile}" }
            .set{ vcfforvqsr }

  process  VariantQualityScoreRecalibration {
    label 'gatk'
    tag "snp + indel"
    publishDir "${params.outdir}/variant/", mode: 'copy'

    input:
    file rawsnp from rawsnp
    file rawindel from rawindel
    file fasta_fai from fasta_fai_VQSR
    file fasta from fasta_VQSR
    file fasta_dict from fasta_dict_VQSR
    file vqsrvcf from vcfforvqsr
    val percent from params.vqsrrate

    output:
    file "filtered_indels_VQSR.vcf" into indelVQSR
    file "filtered_snps_VQSR.vcf" into snpVQSR
    
    script:
    """
    ## make list of input variant files
    ls *.vcf* > myvcf.list

    for input in \$(cat myvcf.list)
    do
       gatk --java-options "-Xmx${task.memory.giga}g" IndexFeatureFile -I \$input
    done 

  
    gatk --java-options "-Xmx${task.memory.giga}g" SelectVariants \
      -R $fasta \
      -V $vqsrvcf \
      --select-type-to-include INDEL \
      -O indels.vcf
    
    gatk --java-options "-Xmx${task.memory.giga}g" SelectVariants \
      -R $fasta \
      -V $vqsrvcf \
      --select-type-to-include SNP \
      -O snps.vcf

    gatk --java-options "-Xmx${task.memory.giga}g" VariantRecalibrator \
        -R $fasta \
        -V $rawsnp \
        -tranche $percent \
        --resource:backgroundSNP,known=true,training=true,truth=true,prior=10.0 snps.vcf \
        -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \
        -mode SNP \
        -O output_snps.recal \
        --tranches-file output_snps.tranches \
        --rscript-file output_snps.plots.R \
        --max-gaussians 4

    gatk --java-options "-Xmx${task.memory.giga}g" ApplyVQSR \
        -R $fasta \
        -V $rawsnp \
        -O pre-filtered_snps_VQSR.vcf \
        --truth-sensitivity-filter-level $percent \
        --tranches-file output_snps.tranches \
        --recal-file output_snps.recal \
        -mode SNP
    
    gatk --java-options "-Xmx${task.memory.giga}g" SelectVariants \
      -R $fasta \
      -V pre-filtered_snps_VQSR.vcf \
      --exclude-filtered \
      -O filtered_snps_VQSR.vcf

    gatk --java-options "-Xmx${task.memory.giga}g" VariantRecalibrator \
      -R $fasta \
      -V $rawindel \
      --trust-all-polymorphic \
      -tranche $percent \
      -an FS -an ReadPosRankSum -an MQRankSum -an QD -an SOR -an DP \
      -mode INDEL \
      --resource:backgroundindel,known=true,training=true,truth=true,prior=10.0 indels.vcf \
      -O cohort_indels.recal \
      --max-gaussians 4 \
      --tranches-file cohort_indels.tranches

    gatk --java-options "-Xmx${task.memory.giga}g" ApplyVQSR \
        -R $fasta \
        -V $rawindel \
        -O pre-filtered_indels_VQSR.vcf \
        --truth-sensitivity-filter-level $percent \
        --tranches-file cohort_indels.tranches \
        --recal-file cohort_indels.recal \
        -mode INDEL
    
    gatk --java-options "-Xmx${task.memory.giga}g" SelectVariants \
      -R $fasta \
      -V pre-filtered_indels_VQSR.vcf \
      --exclude-filtered \
      -O filtered_indels_VQSR.vcf
    """
  }

  process Extract_specific_variant_VQSR{
        // No label ! Launch with PSMN configuration
        tag "$vcf"
        publishDir "${params.outdir}/variant/", mode: 'copy'

        input:
        val qual from params.minglobalqual
        val dpmin from params.mindepth
        file vcf from indelVQSR.concat(snpVQSR)

        output:
        file "*.vcf" into good_variant , good_variant_for_mqc
        file "nb_removed" into removed
        file "*_mqc*" into bar_multiqc

        script:
        """
        $ext_spec $vcf $qual $dpmin
        """
    }

}
else{
  process Extract_specific_variant{
        // No label ! Launch with PSMN configuration
        tag "$vcf"
        //errorStrategy { task.exitStatus == 0 ? 'terminate' : 'ignore' }
        publishDir "${params.outdir}/variant/", mode: 'copy'

        input:
        val qual from params.minglobalqual
        val dpmin from params.mindepth
        file vcf from snp_files.concat(indel_files)

        output:
        file "*.vcf" into good_variant, good_variant_for_mqc, variant_check
        file "*_mqc*" into bar_multiqc
        file "nb_removed" into removed

        script:
        println("${vcf}")
        """
        python $projectDir/extract_specific.py $vcf $qual $dpmin

        if [ -e .vcf ]
        then
          echo "vcf found"
        else
          echo "vcf not found"
        fi
        """
    }
}




///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                   STEP   : Structural variant caller                -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

//file1 = Channel.fromPath(params.reads)
//file2 = Channel.fromPath(params.reads)
process Structural_Variant_calling_breakdancer_step1 {
  label 'breakdancer'
  tag "${pair_id}"
  publishDir "${params.outdir}/structural_variant/"

  input:
  set pair_id, "${pair_id}.bam", "${pair_id}.bam.bai" from non_filtered_bam_files_breakdancer

  output:
  set  pair_id, "breakdancer_${pair_id}.ctx" into breakdancer_files, breackdancer_metasv, breakdancer_step2
  file "${pair_id}_config.cfg" into config_breakdancer

  script:
  //Attention la doc est fausse il ne faut pas utiliser de chevron pour bam2cfg sinon le fichier de config n'est pas bon

  """
  bam2cfg ${pair_id}.bam -o ${pair_id}_config.cfg
  
  breakdancer-max ${pair_id}_config.cfg > breakdancer_${pair_id}.ctx
  """
}

process Structural_Variant_calling_breakdancer_step2 {
  tag "${pair_id}"
  publishDir "${params.outdir}/structural_variant/"

  input :
  set pair_id, file("breakdancer_${pair_id}.ctx") from breakdancer_step2

  output :
  set pair_id, "breakdancer_${pair_id}.vcf" into breakdancer_vcf

  script :
  """
  # Transform ctx into vcf
  python $projectDir/breakdancer2vcf.py -i breakdancer_${pair_id}.ctx -o 'breakdancer_${pair_id}.vcf'
  """
}


// https://github.com/ALLBio/allbiotc2/blob/master/breakdancer/breakdancer2vcf.py


//http://gmt.genome.wustl.edu/packages/pindel/user-manual.html
process Structural_Variant_calling_pindel {
  label 'pindel'
  tag "${pair_id}"
  publishDir "${params.outdir}/structural_variant/", mode: 'copy'

  input:
  file fasta from fasta_pindel.collect()
  file fasta_fai from fasta_fai_pindel.collect()
  set pair_id, bam, bai from non_filtered_bam_pindel
  file config from config_breakdancer.collect()
  val p from params.ploidy

  output:
  file "${pair_id}_SV_pindel*" into pindel_metasv
  file "pindel_${pair_id}.vcf" into pindel_vcf

  script:
  // recupere la taille d'insert avec les fichier config de breackdancer dans la variable leninsert
  // Creation d'un fichier de config pour pindel avec la taille moyenne des inserts 
  """
  leninsert=\$(cut -f9 ${pair_id}_config.cfg | cut -f2 -d ":" | cut -f1 -d ".")

  echo "${bam}	\$leninsert	${pair_id}" > config

  grep ">" $fasta | awk '/^>/ -F " "{  print \$1" $p" }' | sed 's/>//' > fileploidy

  pindel -f $fasta \
  -T ${task.cpus} \
  -i config \
  --Ploidy fileploidy \
  -o ${pair_id}_SV_pindel

  pindel2vcf -r $fasta -R ${fasta.baseName} -P ${pair_id}_SV_pindel -v pindel_${pair_id}.vcf -d 20210101 -G
  """
}

//https://github.com/arq5x/lumpy-sv

process Structural_Variant_calling_Lumpy_step1 {
  tag "${pair_id}"
  publishDir "${params.outdir}/structural_variant/"

  input:
  set pair_id, file("${pair_id}.bam"), file("${pair_id}.bam.bai") from bam_for_lumpy_2

  output:
  set pair_id, file("${pair_id}_sample.discordants.bam"), file("${pair_id}_sample.splitters.bam"), file("${pair_id}.bam") into lumpy_step2

  script:
  """
  # Extract the discordant paired-end alignments.
  samtools view -b -F 1294 ${pair_id}.bam > sample.discordants.unsorted.bam

  samtools view -h ${pair_id}.bam | python $projectDir/extractSplitReads_BwaMem.py -i stdin | samtools view -Sb - > sample.splitters.unsorted.bam
  
  # Sort both alignments
  samtools sort sample.discordants.unsorted.bam -o ${pair_id}_sample.discordants.bam
  samtools sort sample.splitters.unsorted.bam -o ${pair_id}_sample.splitters.bam
  """

}

process Structural_Variant_calling_Lumpy_step2 {
  label 'lumpy'
  tag "${pair_id}"
  publishDir "${params.outdir}/structural_variant/"

  input:
  set pair_id, file("${pair_id}_sample.discordants.bam"), file("${pair_id}_sample.splitters.bam"), file("${pair_id}.bam") from lumpy_step2
  
  output:
  file "Lumpy_${pair_id}.vcf" into lumpy_out
  

  script:

  """
  #Run Lumpy in express mode
  lumpyexpress \
    -B ${pair_id}.bam \
    -S ${pair_id}_sample.splitters.bam\
    -D ${pair_id}_sample.discordants.bam\
    -o Lumpy_${pair_id}.vcf
  """
}

//https://github.com/abyzovlab/CNVnator
process Structural_Variant_calling_CNVnator {
  label 'cnvnator'
  tag "${pair_id}"
  publishDir "${params.outdir}/structural_variant/", mode: 'copy'

  input:
  file fasta from fasta_cnv.collect()
  set pair_id, bam, bai from non_filtered_bam_files_cnvnator

  output:
  file "${pair_id}_CNV.call" into cnvnator_out

  script:
  """
  cnvnator -root file.root -tree $bam

  cnvnator -root file.root -his 100 -fasta $fasta

  cnvnator -root file.root -stat 100

  cnvnator -root file.root -partition 100

  cnvnator -root file.root -call 100  > ${pair_id}_CNV.call
  """
}

// ---------------------
// Breakseq http://bioinform.github.io/breakseq2/ 
// https://hub.docker.com/r/szarate/breakseq2
// ---------------------

//https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4528635/
//http://bioinform.github.io/metasv/
process Group_Structural_Variant_with_Metasv{
        label 'metasv'
        tag "$pair_id"
        publishDir "${params.outdir}/structural_variant/", mode: 'copy'

        input:
        file fasta from fasta_metasv.collect()
        file fastafai from fastafai_metasv.collect()
        file pindelout from pindel_metasv.collect()  
        //file gatkindel from testgatkmetasv.collect() // #--gatk_vcf $gatkindel \
        file cnv from cnvnator_out.collect()
        file lumpy from lumpy_out.collect()
        set pair_id, breakdancerout , bam , bamindex from breackdancer_metasv.join( non_filtered_bam_files_metasv ) 

        output:
        //set pair_id, "${pair_id}_SV.vcf" into metasvout
        file "${pair_id}_SV.vcf" into vcfmetasv_withnonspecific , sv_vcf_forstat
        file "raw_${pair_id}_SV.vcf" into raw_metasv
        val pair_id into id

        script:
        """
        grep -v "IMPRECISE" Lumpy_${pair_id}.vcf  > Lumpy.vcf


        run_metasv.py \
        --num_threads ${task.cpus} \
        --reference $fasta \
        --breakdancer_native $breakdancerout \
        --pindel_native ${pair_id}_SV_pindel* \
        --cnvnator_native ${pair_id}_CNV.call \
        --lumpy_vcf Lumpy.vcf\
        --outdir out \
        --sample $pair_id \
        --filter_gaps \
        --bam $bam \
        --minsvlen 5 \
        --disable_assembly \
        #--spades spades.py \
        #--age age_align \
        --keep_standard_contigs

        gunzip out/variants.vcf.gz
        mv out/variants.vcf raw_${pair_id}_SV.vcf
        grep -v "LowQual" raw_${pair_id}_SV.vcf | grep -v "IMPRECISE" > ${pair_id}_SV.vcf

        """
}


process Find_specific_SV{
        publishDir "${params.outdir}/structural_variant/", mode: 'copy'

        input:
        file SV from vcfmetasv_withnonspecific.collect()

        output:
        file "*filtered_SV.vcf" into vcfmetasv 

        script:
        """
        python $projectDir/extract_specific_SV.py
        """
}

/*
process Prepare_Structural_Variant_calling_GATK {
    label 'gatk'
    
    input:
    file fasta from fasta_Structural_Variant_calling_GATK_prepare.collect()
    file fasta_fai from fasta_fai_Structural_Variant_calling_GATK_prepare.collect()
    file fasta_dict from fasta_dict_Structural_Variant_calling_GATK_prepare.collect()

    output:
    file "*" into file_to_GATK_SV

    script:
    """
    gatk BwaMemIndexImageCreator \
     -I $fasta \
     -O reference.img
    
    gatk FindBadGenomicKmersSpark \
    -R $fasta \
    -O kmers_to_ignore.txt
    """
  }
  
  process Structural_Variant_calling_GATK {
    label 'gatk'
    tag "$pair_id"
    publishDir "${params.outdir}/stuctural_variant/", mode: 'copy'

    input:
    set pair_id, bam_file, bai from bam2GATK_SV
    file fasta_fai from fasta_fai_Structural_Variant_calling_GATK.collect()
    file fasta from fasta_Structural_Variant_calling_GATK.collect()
    file fasta_dict from fasta_dict_Structural_Variant_calling_GATK.collect()
    file vgdjfhhsdkbh from file_to_GATK_SV.collect()

    output:
    file "*vcf" into GATK_SV

    script:
    """
    gatk BwaMemIndexImageCreator \
     -I $fasta \
     -O reference.img
    
    gatk FindBadGenomicKmersSpark \
    -R $fasta \
    -O kmers_to_ignore.txt

    gatk FindBreakpointEvidenceSpark \
     -I $bam_file \
     --aligner-index-image reference.img \
     --kmers-to-ignore kmers_to_ignore.txt \
     -O aligned_contigs.sam

    gatk StructuralVariationDiscoveryPipelineSpark \
     -I $bam_file \
     -R $fasta \
     --aligner-index-image reference.img \
     --kmers-to-ignore kmers_to_ignore.txt \
     --contig-sam-file aligned_contigs.sam \
     -O GATK_${pair_id}.vcf 
    """
  }
*/


///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                   STEP   : Snp effect                               -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

// Peu de docker disponible, aucun pour la derniere version de snpEff 
// voir pour en crer un ?? 
// Voir pour faire option pour database deja crée et lui passer le chemin du rep data ? 
// voir piur utiliser la database deja crée dans snpeff 
// voir pour la localisation du config file ? possibilité d'aller chercher dans un autre repertoire

if (params.annotationname) {
    //http://pcingola.github.io/SnpEff/ss_extractfields/
    process Snpeff_variant_effect {
        label 'snpeff'
        tag "$file_vcf"
        publishDir "${params.outdir}/snpeff/$file_vcf", mode: 'copy'

        input:
        file fa from fasta_Snpeff_variant_effect2.collect()
        file file_vcf from good_variant.flatten().concat(vcfmetasv.flatten())

        output:
        file "snpeff_${file_vcf}" into anno
        file "snpEff_summary.html" into summary
        file "snpEff_genes.txt" into snptxt
        file "tab_snpeff_${file_vcf}" into tabsnpeff

        script:
        """
        snpeff $params.annotationname \
        -v $file_vcf > snpeff_${file_vcf}

        cat snpeff_${file_vcf} | vcfEffOnePerLine.pl | snpsift extractFields -e '.' - "ANN[*].GENE" CHROM POS REF ALT DP "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].BIOTYPE" | uniq -u > tab_snpeff_${file_vcf}
        """
      }
}
else{
    process Snpeff_build_database {
        label 'snpeff'
        tag "$gff"

        input:
        file gff from annotation
        file fa from fasta_snpeff

        output:
        file "snpEff.config" into configsnpeff
        file "data/*" into snpfile

        script:
        """
        mkdir data
        mkdir ./data/${fa.baseName}/

        cp $gff ./data/${fa.baseName}/genes.gff
        cp $fa ./data/${fa.baseName}/sequences.fa

        echo "#Physcomitrium (Physcomitrella) patens (${fa.baseName}, 11/03/2021)" >> snpEff.config
        echo "${fa.baseName}.genome : Physcomitrium patens"  >> snpEff.config

        snpEff build -gff3 -c snpEff.config -v ${fa.baseName}
        """
        }

      //http://pcingola.github.io/SnpEff/ss_extractfields/
    process Snpeff_variant_effect_withgff {
        label 'snpeff'
        tag "$file_vcf"
        publishDir "${params.outdir}/snpeff/$file_vcf", mode: 'copy'

        input:
        file configfile from configsnpeff.collect()
        file fa from fasta_Snpeff_variant_effect.collect()
        file file_vcf from good_variant.flatten()//.concat(vcfmetasv)
        file directorysnpeff from snpfile.collect()

        output:
        file "snpeff_${file_vcf}" into anno
        file "snpEff_summary.html" into summary
        file "snpEff_genes.txt" into snptxt
        file "tab_snpeff_${file_vcf}" into tabsnpeff

        script:

        println file_vcf
        
        """
        snpeff ${fa.baseName} \
        -c $configfile \
        -v $file_vcf > snpeff_${file_vcf}

        cat snpeff_${file_vcf} | vcfEffOnePerLine.pl | snpsift extractFields -e '.' - "ANN[*].GENEID" "ANN[*].GENE" CHROM POS REF ALT DP "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].BIOTYPE" | uniq -u > tab_snpeff_${file_vcf}
        """
      }
}


process Final_process {
      tag "$pair_id"
      publishDir "${params.outdir}/final", mode: 'copy'

      input:
      file fi from tabsnpeff.collect()
      val pair_id from id

      output:
      file "${pair_id}.tsv" into final_files

      script:
      """
      liste_fichiers=`ls *${pair_id}*`

      for fichier in \$liste_fichiers
      do
        head -1 \$fichier > ${pair_id}.tsv
      done
      
      for i in \$liste_fichiers
      do 
       sed '1d;\$d' \$i >> ${pair_id}.tsv
      done
      """
}

process Generate_custom_summary {

      input:
      file snp from snp_for_stat
      file indel from indel_for_stat
      file ff from final_files.collect()
      file sv from sv_vcf_forstat.collect()
      file nb from removed
      file goodvar from good_variant_for_mqc.collect()

      output:
      file "*_mqc*" into to_multiqc

      script:
      //fimpact.write( '{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_temp.split('.')[0],high,moderate,low,modifier) )

      """
#! python3
import plotly.express as px
import pandas as pd
import sys
import os
import glob
import plotly.graph_objects as go

# List all resuts files
files = glob.glob("*.tsv")

fimpact = open("Summary_variant_impact_mqc.txt","w")
ftype = open("Summary_variant_type_mqc.txt","w")
# This would print all the files and directories


totalsnp = 0
totalindel = 0
totalSV = 0

onesample = False 

for i in open("$snp",'r').readlines():
  if not i.startswith('#'):
    totalsnp += 1

for i in open("$indel",'r').readlines():
  if not i.startswith('#'):
    totalindel += 1

if " " in "$sv":
  for f in "$sv".split(' '):
    for i in open(f,'r').readlines():
      if not i.startswith('#'):
        totalSV += 1
else:
  #only one file 
  onesample = True
  for i in open("$sv",'r').readlines():
    if not i.startswith('#'):
      totalSV += 1

print( "total snp", totalsnp)
print( "total indel", totalindel)
print( "total SV", totalSV)

values = []

fimpact.write("Sample\\tHIGH\\tMODERATE\\tLOW\\tMODIFIER\\n")
ftype.write("Sample\\tsnp\\tindel\\tstructural variant\\n")
for file_temp in files:
    print(file_temp)
    df = pd.read_csv(file_temp, sep='\\t')

    #for variant impact
    df_resume = df.groupby('ANN[*].IMPACT').count()['DP']
    try :
      high = df_resume['HIGH']
    except KeyError :
      high = 0
    try :
      moderate = df_resume['MODERATE']
    except KeyError :
      moderate = 0
    try :
      low = df_resume['LOW']
    except KeyError :
      low = 0
    try :
      modifier = df_resume['MODIFIER']
    except KeyError :
      modifier = 0
    fimpact.write( '{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_temp.split('.')[0],high,moderate,low,modifier) )

    #for variant type info
    dftype = df[["CHROM", "POS","REF", "ALT" ]]
    dftypeuniq = dftype.drop_duplicates()
    print(dftypeuniq.describe())
    snp = 0
    indel = 0
    sv = 0
    for index, row in dftypeuniq.iterrows():
      if row['ALT'].startswith('<'):
        sv += 1
      elif (len(row['REF']) ==	len(row['ALT'])):
        snp += 1
      elif (len(row['REF']) != len(row['ALT'])):
        if "," in row['ALT']:
          snp += 1
        else:
          indel += 1
    ftype.write( '{}\\t{}\\t{}\\t{}\\n'.format(file_temp.split('.')[0],snp,indel,sv) )


#for piechart


lab = []
val = []

if " " in "$goodvar":
  for f in "$goodvar".split(' '):
    nb = 0
    for i in open(f,'r').readlines():
      if not i.startswith('#'):
        nb += 1
    lab.append( f )
    val.append( nb )



total = totalindel+totalsnp+totalSV
specific = total - sum(val)


lab.append("shared")
val.append(specific)

print( lab)
print( val)

fig = go.Figure(data=[go.Pie(labels=lab, values=val)])
fig.write_html("Specific_vs_shared_variants_mqc.html")
"""

}



///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                       STEP   : MULTIQC                          -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////


if (params.readsinbam) {
 process MultiQC_withbamfile {
     label "multiQC"
     publishDir "${params.outdir}/multiQC", mode: 'copy'

     input:
     file report_flagstat from flagstat_files.collect()
     //file report_gatk from gatkmetric_files.collect()
     file report_custom from to_multiqc.collect()
     file cov from cov_multiqc.collect()
     file bar from bar_multiqc.collect()

     output:
     file "*multiqc_*" into multiqc_report

     script:
     """
     multiqc .
     """
  }
}
else{
  process MultiQC {
     label "multiQC"
     publishDir "${params.outdir}/multiQC", mode: 'copy'

     input:
     file report_fastqc from fastq_report_files.collect()
     file report_flagstat from flagstat_files.collect()
     file report_mapping from mapping_report_files.collect()
     file report_picard from picardmetric_files.collect()
     //file report_gatk from gatkmetric_files.collect()
     file report_custom from to_multiqc.collect()
     file cov from cov_multiqc.collect()
     file bar from bar_multiqc.collect()

     output:
     file "*multiqc_*" into multiqc_report

     script:
     """
     multiqc .
     """
}
}


///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/* --                                                                     -- */
/* --                       NF-CORE HEADER                                -- */
/* --                                                                     -- */
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

def nfcoreHeader() {
    // Log colors ANSI codes
    c_reset =  "\033[0m";
    c_dim = "\033[2m";
    c_black = "\033[0;30m";
    c_green = "\033[0;32m";
    c_yellow = "\033[0;33m";
    c_blue = "\033[0;34m";
    c_purple = "\033[0;35m";
    c_cyan = "\033[0;36m";
    c_white ="\033[0;37m";

    return """-${c_dim}-------------------------------------------------------------------------${c_reset}-
${c_purple}  @@      @@@   @@     @@@@@@@@   @@@@      @@    @@@     @@@@  @@@@@@@@@@ 
${c_purple}    @     @@   @  @     @@    @@    @      @  @     @ @@    @       @@ 
${c_purple}     @   @@   @@@@@@    @@@@@,      @     @@@@@@    @   @@  @       @@ 
${c_green}      @ @@   @     @@   @@    @     @    @     @@   @     @ @       @@ 
${c_green}       @@   @@@    @@@  @@     @  @@@@  @@      @@ @@       @       @@ 
${c_purple}                                                                          
${c_cyan}                           @@@.       @      @@@     @@@     @@@@@@@   @@@@@@@
${c_cyan}                        @       @    @ @      @       @       @@       @     @@ 
${c_cyan}                       @@           @   @     @       @       @@@@@@   @@@@@ 
${c_blue}                        @@         @@@@@@@    @       @       @@       @@   @     
${c_blue}                          @@@@@   @      @@   @@@@@   @@@@@  @@@@@@@   @     @@
${c_yellow}                                                       ROMUALD MARIN PIPELINE${c_reset}
-${c_dim}-------------------------------------------------------------------------${c_reset}-
    """.stripIndent()
}